---
title: "人工智能公平困境：为什么我们不能拥有一切"
published: 2025-06-12
description: "解释为什么机器学习中三个最突出的公平性指标相互不兼容，并探讨构建更公平模型的因果方法。"
image: ''
tags: [AI, Machine Learning, Fairness, Ethics]
category: Engineering
draft: false
lang: "zh_CN"
originalSlug: "ML_fairness"
series:
    name: "AI Foundations"
    order: 3

---

机器学习模型越来越多地对我们的生活做出关键决策，从贷款申请到工作推荐，甚至犯罪预测。 但存在一个巨大的问题：这些模型从历史数据中学习，而这些数据通常充满社会偏见。 经过有偏见数据训练的人工智能不仅会学习这些偏见，还会学习这些偏见。 它可以放大它们。 

2016 年 ProPublica 的一份著名报告显示，COMPAS 累犯预测工具对非裔美国被告存在偏见。 这引发了人工智能社区定义和衡量“公平性”的重大推动力。 问题？ 三个最突出的公平性指标**相互不兼容**。 一篇开创性的论文不仅解释了这是真的，而且从基本的结构层面解释了为什么它是真的。 

---

 ## 公平的三张面孔 ⚖️

 为了理解这个问题，我们需要了解定义公平的主要竞争者。 本文重点关注三个流行的指标：

 * **人口统计平等**：该指标坚持认为模型的预测必须独立于种族或性别等敏感属性。 简而言之，所有群体的积极成果（例如获得贷款）的比率应该相同。 

* **均衡赔率**：此指标要求模型的每个结果在不同组中的准确性相同。 例如，男性和女性的真阳性率和假阳性率应该相同。 

* **预测奇偶性**：该指标确保对于任何给定的预测，其正确的概率对于所有组都是相同的。 例如，如果模型预测一个人将偿还贷款，那么该群体的实际还款率应该在所有种族中保持一致。 

“不可能定理”证明，除了一些微不足道的情况外，模型无法同时满足所有这三个指标。 这让开发人员陷入困境，迫使他们以牺牲其他人的利益为代价来选择优先考虑哪种公平定义。 

---

 ## 因果解释：为什么它们不能共存

 该论文的主要贡献是使用**因果图**解释了这种不可能性。 它不只是查看统计数据，而是检查每个公平指标所需的底层数据生成结构。 让我们将 **敏感属性** 表示为`A`，**真实结果**为`Y`，**模型的预测**为`Ŷ`。 

之间的因果关系`A`,`Y`， 和`Ŷ`每个指标必须采用不同的结构来保存：

 * 对于 **人口平等** (`Ŷ`独立于`A`），当预测和敏感属性之间的路径自然被阻塞时`Y`是一个“碰撞器”。

:::note
[Causal Diagram for Demographic Parity]
`A -> Y <- Ŷ`
 * 对于 **均等赔率**（给定“Y”，“Ŷ”独立于“A”），观察真实结果“Y”会阻塞“A”和“Ŷ”之间的路径。
:::注意[均等赔率的因果图]`A -> Y -> Ŷ`
    :::

* For **Predictive Parity** (`Y` is independent of `A` given `Ŷ`), observing the prediction `Ŷ` blocks the path.
    :::注意[预测奇偶校验的因果图]`A -> Ŷ -> Y`系列：
 名称：《人工智能基础》
 订单：3

 从这些图中可以清楚地看出，每个指标所需的数据的基本结构是不同的且相互排斥的。 您根本无法拥有一个数据生成过程来同时满足所有这些要求。 问题不在于学习算法，而在于学习算法。 这是数据本身的基本约束。 

---

 ## 前进的新道路：通过纠正实现公平💡

 那么，如果完全公平不可能，我们该怎么办？ 作者认为我们需要改变目标。 

标准机器学习的目标是**经验风险最小化（ERM）**，这意味着模型因完美匹配（通常有偏差的）训练数据中的标签而获得奖励。 但如果目标不是预测“历史”标签，而是预测“公平”标签呢？ 

一个新的因果框架引入了**“校正变量”，`C`** 已提议。 你可以想到`C`作为开关。 该变量，受敏感属性影响`A`，确定模型的预测是否`Ŷ`应遵循真实标签`Y`或不同的“公平”功能。

:::note
[Causal Diagram for Fairness Through Correction]
A -> C
C -> Ŷ
Y -> Ŷ
:::本质上，对于历史上一直处于优势的群体来说，该模型可能会照常进行（`C=1`）。 但对于弱势群体来说，修正变量可能会翻转（`C=0`），导致模型偏离历史数据以产生更公平的结果。 这种方法有几个优点：

 *它承认公平需要积极**偏离有偏见的历史模式**。 
* 它允许从业者调整超参数，以根据数据的不公平程度来决定**需要多少偏差**。 
* 它可以同时满足人口统计和平等赔率的宽松版本，以修正变量为条件`C`。 

通过重新构建问题，这种因果方法提供了一个强大的工具来构建模型，这些模型不仅反映了世界的本来面目，而且有助于创建一个更符合我们不断发展的公平观念的世界。