---
title: "人工智能公平困境：為什麼我們不能擁有一切"
published: 2025-06-12
description: "解釋為什麼機器學習中三個最突出的公平性指標相互不兼容，並探討構建更公平模型的因果方法。"
image: ''
tags: [AI, Machine Learning, Fairness, Ethics]
category: Engineering
draft: false
lang: "zh_TW"
originalSlug: "ML_fairness"
series:
    name: "AI Foundations"
    order: 3

---

機器學習模型越來越多地對我們的生活做出關鍵決策，從貸款申請到工作推薦，甚至犯罪預測。 但存在一個巨大的問題：這些模型從歷史數據中學習，而這些數據通常充滿社會偏見。 經過有偏見數據訓練的人工智能不僅會學習這些偏見，還會學習這些偏見。 它可以放大它們。 

2016 年 ProPublica 的一份著名報告顯示，COMPAS 累犯預測工具對非裔美國被告存在偏見。 這引發了人工智能社區定義和衡量“公平性”的重大推動力。 問題？ 三個最突出的公平性指標**相互不兼容**。 一篇開創性的論文不僅解釋了這是真的，而且從基本的結構層面解釋了為什麼它是真的。 

---

## 公平的三張面孔 ⚖️

為了理解這個問題，我們需要了解定義公平的主要競爭者。 本文重點關註三個流行的指標：

* **人口統計平等**：該指標堅持認為模型的預測必須獨立於種族或性別等敏感屬性。 簡而言之，所有群體的積極成果（例如獲得貸款）的比率應該相同。 

* **均衡賠率**：此指標要求模型的每個結果在不同組中的準確性相同。 例如，男性和女性的真陽性率和假陽性率應該相同。 

* **預測奇偶性**：該指標確保對於任何給定的預測，其正確的概率對於所有組都是相同的。 例如，如果模型預測一個人將償還貸款，那麼該群體的實際還款率應該在所有種族中保持一致。 

“不可能定理”證明，除了一些微不足道的情況外，模型無法同時滿足所有這三個指標。 這讓開發人員陷入困境，迫使他們以犧牲其他人的利益為代價來選擇優先考慮哪種公平定義。 

---

## 因果解釋：為什麼它們不能共存

該論文的主要貢獻是使用**因果圖**解釋了這種不可能性。 它不只是查看統計數據，而是檢查每個公平指標所需的底層數據生成結構。 讓我們將 **敏感屬性** 表示為`A`，**真實結果**為`Y`，**模型的預測**為`Ŷ`。 

之間的因果關係`A`,`Y`， 和`Ŷ`每個指標必須採用不同的結構來保存：

* 對於 **人口平等** (`Ŷ`獨立於`A`），當預測和敏感屬性之間的路徑自然被阻塞時`Y`是一個“碰撞器”。

:::note
[Causal Diagram for Demographic Parity]
`A -> Y <- Ŷ`
* 對於 **均等賠率**（給定“Y”，“Ŷ”獨立於“A”），觀察真實結果“Y”會阻塞“A”和“Ŷ”之間的路徑。
:::注意[均等賠率的因果圖]`A -> Y -> Ŷ`
    :::

* For **Predictive Parity** (`Y` is independent of `A` given `Ŷ`), observing the prediction `Ŷ` blocks the path.
    :::注意[預測奇偶校驗的因果圖]`A -> Ŷ -> Y`系列：
名稱：《人工智能基礎》
訂單：3

從這些圖中可以清楚地看出，每個指標所需的數據的基本結構是不同的且相互排斥的。 您根本無法擁有一個數據生成過程來同時滿足所有這些要求。 問題不在於學習算法，而在於學習算法。 這是數據本身的基本約束。 

---

## 前進的新道路：通過糾正實現公平💡

那麼，如果完全公平不可能，我們該怎麼辦？ 作者認為我們需要改變目標。 

標準機器學習的目標是**經驗風險最小化（ERM）**，這意味著模型因完美匹配（通常有偏差的）訓練數據中的標籤而獲得獎勵。 但如果目標不是預測“歷史”標籤，而是預測“公平”標籤呢？ 

一個新的因果框架引入了**“校正變量”，`C`** 已提議。 你可以想到`C`作為開關。 該變量，受敏感屬性影響`A`，確定模型的預測是否`Ŷ`應遵循真實標籤`Y`或不同的“公平”功能。

:::note
[Causal Diagram for Fairness Through Correction]
A -> C
C -> Ŷ
Y -> Ŷ
:::本質上，對於歷史上一直處於優勢的群體來說，該模型可能會照常進行（`C=1`）。 但對於弱勢群體來說，修正變量可能會翻轉（`C=0`），導致模型偏離歷史數據以產生更公平的結果。 這種方法有幾個優點：

*它承認公平需要積極**偏離有偏見的歷史模式**。 
* 它允許從業者調整超參數，以根據數據的不公平程度來決定**需要多少偏差**。 
* 它可以同時滿足人口統計和平等賠率的寬鬆版本，以修正變量為條件`C`。 

通過重新構建問題，這種因果方法提供了一個強大的工具來構建模型，這些模型不僅反映了世界的本來面目，而且有助於創建一個更符合我們不斷發展的公平觀念的世界。