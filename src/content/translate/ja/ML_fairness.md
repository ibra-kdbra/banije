---
title: "AI の公平性のジレンマ: なぜすべてを手に入れることができないのか"
published: 2025-06-12
description: "機械学習における 3 つの最も有名な公平性メトリクスが相互に互換性がない理由の説明と、より公平なモデルを構築するための因果的アプローチについて説明します。"
image: ''
tags: [AI, Machine Learning, Fairness, Ethics]
category: Engineering
draft: false
lang: "ja"
originalSlug: "ML_fairness"
series:
    name: "AI Foundations"
    order: 3

---

機械学習モデルは、ローンの申し込みから仕事の推薦、さらには犯罪の予測に至るまで、私たちの生活に関する重要な決定をますます行っています。 しかし、大きな問題があります。これらのモデルは過去のデータから学習しており、そのデータには社会的な偏見が含まれていることがよくあります。 バイアスのあるデータに基づいてトレーニングされた AI は、単にこれらのバイアスを学習するだけではありません。 それはそれらを増幅する可能性があります。

2016 年の有名なプロパブリカの報告書では、COMPAS の再犯予測ツールがアフリカ系アメリカ人の被告に対して偏っていたことが明らかになりました。 これは、AI コミュニティで「公平性」を定義し、測定するという大きな推進を引き起こしました。 問題？ 最も著名な 3 つの公平性指標は **相互に互換性がありません**。 画期的な論文は、これが真実であることだけでなく、基本的かつ構造的なレベルからそれが真実であることを説明しています。

---

## 公平性の 3 つの顔 ⚖️

 この問題を理解するには、公平性を定義する主な候補者を知る必要があります。 この文書では、次の 3 つの一般的な指標に焦点を当てています。

* **人口平等**: この指標は、モデルの予測が人種や性別などの機密属性から独立している必要があることを主張します。 簡単に言えば、肯定的な結果 (融資を受けるなど) の割合はすべてのグループで同じである必要があります。

* **均等化オッズ**: このメトリクスでは、各結果についてモデルの精度が異なるグループ間で等しいことが必要です。 たとえば、真陽性率と偽陽性率は男性と女性で同じである必要があります。

* **予測パリティ**: このメトリクスは、特定の予測について、それが正しい確率がすべてのグループで同じであることを保証します。 たとえば、ある人がローンを返済するとモデルが予測する場合、そのグループの実際の返済率はすべての人種にわたって一貫しているはずです。

「不可能性定理」は、些細な場合を除いて、モデルがこれら 3 つの指標すべてを同時に満たすことができないことを証明しています。 このため開発者は窮地に陥り、他のものを犠牲にして公平性のどの定義を優先するかを選択する必要に迫られています。

---

## 因果関係の説明: なぜ共存できないのか

 この論文の主な貢献は、**因果関係図**を使用してこの不可能性を説明したことです。 単に統計を調べるのではなく、各公平性メトリクスに必要な基礎となるデータ生成構造を調べます。 **機密属性**を次のように表します。`A`、**本当の結果**として`Y`、**モデルの予測**は次のようになります。`Ŷ`。

間の因果関係`A`、`Y`、 そして`Ŷ`保持するメトリクスごとに異なる構造にする必要があります。

* **人口平等**の場合 (`Ŷ`から独立しています`A`)、次の場合、予測と機密属性の間のパスは自然にブロックされます。`Y`「コライダー」です。

:::note
[人口平等の因果関係図]
`A -> Y <- Ŷ`
:::

* **等化オッズ** (`Ŷ` は、`Y` が与えられた場合、`A` から独立しています) の場合、真の結果 `Y` を観察すると、`A` と `Ŷ` の間のパスがブロックされます。

:::note
[均等化されたオッズの因果関係図]
`A -> Y -> Ŷ`
:::

* For **Predictive Parity** (`Y` is independent of `A` given `Ŷ`), observing the prediction `Ŷ` blocks the path.

:::note
[予測パリティの因果図]
`A -> Ŷ -> Y`シリーズ:
 名称：「AI財団」
 注文: 3
:::

 これらの図から、各メトリクスに必要なデータの基本構造が異なり、相互に排他的であることが明らかです。 これらすべてを同時に満たす単一のデータ生成プロセスを使用することはできません。 問題は学習アルゴリズムではありません。 これはデータ自体の基本的な制約です。

---

## 進むべき新たな道: 訂正による公平性 💡

 では、完全な公平性が不可能な場合、どうすればよいのでしょうか? 著者は、目標を変更する必要があると主張します。

標準的な機械学習は **経験的リスク最小化 (ERM)** を目的としています。これは、モデルが (バイアスのある) トレーニング データ内のラベルと完全に一致すると報酬が与えられることを意味します。 しかし、目的が *歴史的* ラベルを予測することではなく、*公正な* ラベルを予測することだったらどうなるでしょうか?

 **「補正変数」を導入する新しい因果関係のフレームワーク`C`**が提案されています。 考えられるのは`C`スイッチとして。 この変数は機密属性の影響を受けます`A`、モデルの予測が正しいかどうかを決定します。`Ŷ`本当のラベルに従うべきです`Y`または別の「公平性」関数。

:::note[訂正による公平性の因果関係図]
A -> C
C -> Ŷ
Y -> Ŷ
:::

基本的に、歴史的に有利な立場にあったグループの場合、モデルは通常どおり進む可能性があります (`C=1`）。 しかし、不利な立場にあるグループの場合、補正変数が反転する可能性があります (`C=0`)、より公平な結果を生み出すためにモデルが過去のデータから逸脱する原因となります。 このアプローチにはいくつかの利点があります。

* 公平性を保つためには、**偏った歴史的パターンから積極的に逸脱する**ことが必要であることを認めています。
* これにより、実務者はハイパーパラメータを調整して、データがどの程度不公平であるかに基づいて **必要な偏差の量** を決定できます。
* 補正変数を条件として、人口パリティと均等化オッズの緩いバージョンを同時に満たすことができます。`C`。

問題を再構成することにより、この因果的アプローチは、世界をありのままに反映するだけでなく、私たちの進化する公平性の概念により沿った世界を作成するのに役立つモデルを構築するための強力なツールを提供します。
