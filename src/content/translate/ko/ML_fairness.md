---
title: "AI 공정성 딜레마: 우리가 모든 것을 가질 수 없는 이유"
published: 2025-06-12
description: "기계 학습에서 가장 두드러진 세 가지 공정성 지표가 상호 호환되지 않는 이유에 대한 설명과 보다 공정한 모델을 구축하기 위한 인과적 접근 방식을 살펴봅니다."
image: ''
tags: [AI, Machine Learning, Fairness, Ethics]
category: Engineering
draft: false
lang: "ko"
originalSlug: "ML_fairness"
series:
    name: "AI Foundations"
    order: 3

---

기계 학습 모델은 대출 신청부터 직업 추천, 범죄 예측까지 우리 삶에 대한 중요한 결정을 점점 더 많이 내리고 있습니다. 그러나 큰 문제가 있습니다. 이러한 모델은 과거 데이터를 통해 학습하며 해당 데이터는 종종 사회적 편견으로 가득 차 있습니다. 편향된 데이터로 훈련된 AI는 이러한 편향만 학습하는 것이 아닙니다. 그것은 그것들을 증폭시킬 수 있습니다.

유명한 2016년 ProPublica 보고서에서는 COMPAS의 재범 예측 도구가 아프리카계 미국인 피고인에 대해 편견을 갖고 있는 것으로 나타났습니다. 이는 AI 커뮤니티에서 "공정성"을 정의하고 측정하려는 주요 추진을 촉발했습니다. 문제? 가장 두드러진 세 가지 공정성 지표는 **상호 호환되지 않습니다**. 획기적인 논문에서는 이것이 사실이라는 *사실*뿐만 아니라 기본적이고 구조적인 수준에서 *왜* 사실인지 설명합니다.

---

## 공정성의 세 가지 얼굴 ⚖️

문제를 이해하려면 공정성을 정의하는 주요 경쟁자를 알아야 합니다. 이 백서는 세 가지 인기 있는 지표에 중점을 두고 있습니다.

* **인구통계학적 동등성**: 이 측정항목은 모델의 예측이 인종이나 성별과 같은 민감한 속성과 무관해야 함을 나타냅니다. 간단히 말해서 긍정적인 결과(예: 대출 받기)의 비율은 모든 그룹에서 동일해야 합니다.

* **균등 확률**: 이 측정항목을 사용하려면 모델의 정확도가 각 결과에 대해 여러 그룹에서 동일해야 합니다. 예를 들어, 진양성률과 위양성률은 남성과 여성 모두 동일해야 합니다.

* **예측 패리티**: 이 측정항목은 특정 예측에 대해 예측이 정확할 확률이 모든 그룹에서 동일함을 보장합니다. 예를 들어, 모델이 어떤 사람이 대출금을 상환할 것이라고 예측하는 경우 해당 그룹의 실제 상환율은 모든 인종에 걸쳐 일관되어야 합니다.

"불가능 정리"는 사소한 경우를 제외하고는 모델이 이 세 가지 지표를 동시에 모두 만족할 수 없음을 증명합니다. 이로 인해 개발자는 다른 사람을 희생하면서 우선순위를 정할 공정성 정의를 선택해야 하는 곤경에 처하게 되었습니다.

---

## 원인 설명: 공존할 수 없는 이유

이 논문의 주요 기여는 **인과관계 도표**를 사용하여 이러한 불가능성을 설명하는 것입니다. 단순히 통계만 보는 것이 아니라 각 공정성 지표에 필요한 기본 데이터 생성 구조를 검사합니다. **민감한 속성**을 다음과 같이 표시하겠습니다.`A`, **실제 결과**는 다음과 같습니다.`Y`, **모델의 예측**은 다음과 같습니다.`Ŷ`.

사이의 인과관계`A`,`Y`, 그리고`Ŷ`유지하려면 측정항목마다 다르게 구성되어야 합니다.

* **인구학적 동등성**의 경우(`Ŷ`독립하다`A`), 예측과 민감한 속성 사이의 경로는 다음과 같은 경우 자연스럽게 차단됩니다.`Y`"콜라이더"입니다.

:::note[참고]
[Causal Diagram for Demographic Parity]
`A -> Y <- Ŷ`
:::

* **균등 확률**(`Ŷ`은 `Y`가 주어지면 `A`와 독립적임)의 경우 실제 결과 `Y`를 관찰하면 `A`와 `Ŷ` 사이의 경로가 차단됩니다.

:::note[참고]
[균등 확률의 인과관계도]
`A -> Y -> Ŷ`
:::

* **예측 패리티**(`Y`는 `Ŷ`가 주어지면 `A`와 독립적임)의 경우 예측 `Ŷ`를 관찰하면 경로가 차단됩니다.

:::note[참고]
[예측 패리티의 원인 다이어그램]
`A -> Ŷ -> Y`
:::

이러한 다이어그램을 보면 각 지표에 필요한 데이터의 기본 구조가 다르며 상호 배타적이라는 것이 분명해졌습니다. 동시에 모든 것을 만족시키는 단일 데이터 생성 프로세스를 가질 수는 없습니다. 문제는 학습 알고리즘이 아닙니다. 이는 데이터 자체의 근본적인 제약입니다.

---

## 앞으로 나아가는 새로운 길: 교정을 통한 공정성 💡

그렇다면 완벽한 공정성이 불가능하다면 어떻게 해야 할까요? 저자는 목표를 바꿔야 한다고 주장한다.

표준 기계 학습은 **ERM(경험적 위험 최소화)**을 목표로 합니다. 이는 모델이 (종종 편향된) 학습 데이터의 레이블과 완벽하게 일치하면 보상을 받는다는 것을 의미합니다. 하지만 목표가 *역사적* 라벨을 예측하는 것이 아니라 *공정한* 라벨을 예측하는 것이라면 어떨까요?

**"수정 변수"를 도입하는 새로운 인과 프레임워크,`C`** 제안되었습니다. 당신은 생각할 수 있습니다`C`스위치로. 민감한 속성의 영향을 받는 이 변수는`A`, 모델의 예측 여부를 결정합니다.`Ŷ`실제 라벨을 따라야합니다`Y`또는 다른 "공정성" 기능.

:::note[참고]
[Causal Diagram for Fairness Through Correction]
A -> C
C -> Ŷ
Y -> Ŷ
:::

본질적으로, 역사적으로 이점을 누린 그룹의 경우 모델은 평소대로 진행될 수 있습니다(`C=1`). 그러나 불리한 집단의 경우 수정 변수가 뒤집힐 수 있습니다(`C=0`), 이로 인해 모델이 과거 데이터에서 벗어나 보다 공평한 결과를 생성하게 됩니다. 이 접근 방식에는 다음과 같은 몇 가지 장점이 있습니다.

* 공정성을 위해서는 적극적으로 **편향된 역사적 패턴에서 벗어나**야 한다는 점을 인정합니다.
* 이를 통해 실무자는 초매개변수를 조정하여 데이터가 얼마나 불공평한지에 따라 **얼마나 많은 편차가 필요한지** 결정할 수 있습니다.
* 수정 변수에 따라 인구통계학적 패리티와 균등 확률의 느슨한 버전을 함께 만족시킬 수 있습니다.`C`.

문제를 재구성함으로써 이러한 인과적 접근 방식은 세상을 있는 그대로 반영할 뿐만 아니라 진화하는 공정성 개념에 더욱 부합하는 세상을 만드는 데 도움이 되는 모델을 구축하기 위한 강력한 도구를 제공합니다.
